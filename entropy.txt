Entropy H(X) is the amount of uncertainty about a random variable X.
H(X) = -sum_x  pX(x)log pX(x), where the logarithm is base 2, which corresponds to measuring entropy in bits. 
We take: 0 log 0 = 0.

Properties:
-   The entropy of a constant is 0. Since a constant is not random, it has no uncertainty and the entropy is 0.

-   Entropy is non-negative. 0 ≤ H(X) ≤ minimum number of bits to represent X, on average.

Binary entropy function: h(p) = −p log p − (1 − p) log(1 − p)
An important random variable is the binary random variable. Let X with X = {0, 1} have distribution: 
pX(x) = p if x=0 and pX(x) = 1-p if x=1. Then H(X) = −p log p−(1−p)log(1−p).

Uniform distribution maximizes entropy. The uniform distribution is pX(x)= 1/|X| for x∈X, and so H(X) = sum (1/|X| )( log|X| ) = log|X|.